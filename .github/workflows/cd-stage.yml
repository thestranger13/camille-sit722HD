# camille-sit722/.github/workflows/cd-stage.yml

name: Stage 2 CD Pipeline - Deployment to Staging Environment 

# Trigger the workflow only when stage 1 i.e. ci-test.yml is successful on the 'testing' branch
on:
  workflow_run:
    workflows: ["Stage 1 CI Pipeline - Test and Push Images to ACR"]
    types: [completed]

# Global environment variables that can be used across jobs
env:
  # ACR Login Server (e.g., myregistry.azurecr.io)
  ACR_LOGIN_SERVER: ${{ vars.ACR_LOGIN_SERVER }}  
  # To adopt ci-test tags 
  IMAGE_TAG: latest

jobs:
  # Job: Deployment to staging environment 
  deploy_staging:
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    environment: staging
    steps: 
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha }}

      # 1. Azure login using a Service Principal secret
      - name: Azure Login
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      # 2. Set AKS 
      - name: Set AKS context
        uses: azure/aks-set-context@v4
        with:
            resource-group: ${{ vars.AKS_RESOURCE_GROUP }}
            cluster-name: ${{ vars.AKS_CLUSTER_NAME }}

      # 3. Create namespace i.e. staging-run.id and the relevant baseline manifests
      - name: Create 'staging' namespace
        id: ns
        run: |
            NS="staging-${{ github.run_id }}"
            echo "ns=$NS" >> "$GITHUB_OUTPUT"
            kubectl create namespace "$NS"

      # 4. Apply all Kubernetes 
      - name: Apply ConfigMaps & Secrets
        run: |
            NS="${{ steps.ns.outputs.ns }}"
            kubectl apply -n "$NS" -f k8s/configmaps.yaml
            kubectl apply -n "$NS" -f k8s/secrets.yaml
            kubectl apply -n "$NS" -f k8s/rabbitmq.yaml
            kubectl apply -n "$NS" -f k8s/product-db.yaml
            kubectl apply -n "$NS" -f k8s/order-db.yaml
            kubectl apply -n "$NS" -f k8s/customer-db.yaml

      - name: Deploy services (Product / Order / Customer)
        run: |
            NS="${{ steps.ns.outputs.ns }}"
            kubectl apply -n "$NS" -f k8s/product-service.yaml
            kubectl apply -n "$NS" -f k8s/order-service.yaml
            kubectl apply -n "$NS" -f k8s/customer-service.yaml

      - name: Deploy frontend
        run: |
            NS="${{ steps.ns.outputs.ns }}"
            kubectl apply -n "$NS" -f k8s/frontend.yaml

      - name: Ensure set to images in ACR
        run: |
          NS="${{ steps.ns.outputs.ns }}"
          ACR="${{ env.ACR_LOGIN_SERVER }}"
          kubectl -n "$NS" set image deploy/product-service-w10d2  product-service-container=$ACR/product_service:latest
          kubectl -n "$NS" set image deploy/order-service-w10d2    order-service-container=$ACR/order_service:latest
          kubectl -n "$NS" set image deploy/customer-service-w10d2 customer-service-container=$ACR/customer_service:latest
          kubectl -n "$NS" set image deploy/frontend               frontend-container=$ACR/frontend:latest

      # 5. Wait for the DBs, RabbitMQ and services to be ready with a timeout 
      - name: Wait for DBs & RabbitMQ ready
        run: |
            NS="${{ steps.ns.outputs.ns }}"
            kubectl -n "$NS" rollout status deploy/product-db-deployment-w10d2   --timeout=180s
            kubectl -n "$NS" rollout status deploy/order-db-deployment-w10d2     --timeout=180s
            kubectl -n "$NS" rollout status deploy/customer-db-deployment-w10d2  --timeout=180s
            kubectl -n "$NS" rollout status deploy/rabbitmq-deployment-w10d2     --timeout=180s

      - name: Wait for services ready
        run: |
            NS="${{ steps.ns.outputs.ns }}"
            kubectl -n "$NS" rollout status deploy/product-service-w10d2   --timeout=240s
            kubectl -n "$NS" rollout status deploy/order-service-w10d2     --timeout=240s
            kubectl -n "$NS" rollout status deploy/customer-service-w10d2  --timeout=240s
            kubectl -n "$NS" rollout status deploy/frontend                --timeout=240s

            # ensure that each service endpoint is ready before moving to next step 
            for svc in product-service-w10d2 order-service-w10d2 customer-service-w10d2; do
              echo "Waiting for $svc endpoints..."
              for i in {1..12}; do
                ready=$(kubectl -n "$NS" get endpoints $svc \
                  -o jsonpath='{.subsets[0].addresses[0].ip}' 2>/dev/null || true)
                if [ -n "$ready" ]; then
                  echo "$svc has endpoints -> $ready"
                  break
                fi
                echo "Still waiting for $svc... ($i/12)"
                sleep 10
              done
            done

      # 6. Conduct an acceptance test i.e. smoke test
      - name: Run acceptance test
        run: |
            NS="${{ steps.ns.outputs.ns }}"
            kubectl -n "$NS" run smoke --rm -i --restart=Never \
              --image=curlimages/curl:8.8.0 -- \
              sh -eu -c '
              # Return 200 for successful health checks, if not the job will fail 
                check_200 () {
                  url="$1"; name="$2";
                  code=$(curl -sS -o /dev/null -w "%{http_code}" "$url" || true)
                  if [ "$code" != "200" ]; then
                    echo "[FAIL] $name: expected 200, got $code ($url)"
                    exit 1
                  fi
                  echo "[OK] $name -> 200"
                }

                # Basic health checks
                check_200 http://product-service-w10d2:8000/health   "product-service /health"
                check_200 http://order-service-w10d2:8001/health     "order-service /health"
                check_200 http://customer-service-w10d2:8002/health  "customer-service /health"
              '

      # Obtain frontend address for DAST scan 
      - name: Obtain frontend address
        run: |
          NS="${{ steps.ns.outputs.ns }}"
          for i in {1..30}; do
            host=$(kubectl -n "$NS" get svc frontend-w10d2 \
              -o jsonpath='{.status.loadBalancer.ingress[0].hostname}{.status.loadBalancer.ingress[0].ip}')
            if [ -n "$host" ]; then
              echo "FRONTEND_LB_HOST=$host" >> $GITHUB_ENV
              echo "Frontend LB: $host"
              break
            fi
            echo "Waiting for LB external address... ($i/30)"; sleep 10
          done
          [ -z "${host}" ] && echo "No LB address assigned" && exit 1

      # 7. Run DAST on live application (OWASP ZAP baseline scan)
      - name: Run DAST using ZAP  
        run: |
          docker run --rm -v "$PWD:/zap/wrk" owasp/zap2docker-stable \
            zap-baseline.py -t "http://${FRONTEND_LB_HOST}" \
              -m 0 -I -J /zap/wrk/zap.json -r /zap/wrk/zap.html \
              -z "-config ajaxSpider.enable=false -config spider.maxDuration=0"
          
      # DAST to fail on any high alerts
      - name: Fail if any High alerts
        run: |
          sudo apt-get update -y && sudo apt-get install -y jq
          COUNT=$(jq '[.site[].alerts[] | select(.risk=="High")] | length' ./zap.json 2>/dev/null || echo 0)
          echo "ZAP High alerts: $COUNT"
          if [ "$COUNT" -gt 0 ]; then
            echo "Fail due to $COUNT High alerts"
            exit 1
          fi

      # 8. DAST Report with ZAP Generated 
      - name: Upload ZAP HTML report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: zap-report
          path: ./zap.html

      # 9. Destroy temporary namespace
      - name: Destroy temporary namespace
        if: always()
        run: |
          kubectl delete namespace "${{ steps.ns.outputs.ns }}" --wait=false